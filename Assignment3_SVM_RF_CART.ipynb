{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use project clean data - OATH Violations\n",
    "\n",
    "df = pd.read_csv('processed_data/2016_FINAL_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[~(df['GEOID'].isnull()) & ~(df['MEDIAN_PERSON_AGE'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['YEARS_OLD'] = df['YEARS_OLD'].apply(lambda x: 0 if x == 2017. else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Determine X and Y and test/train values\n",
    "df_y = df.ix[:,0]\n",
    "df_x = df.ix[:, 5:27]\n",
    "X = df_x.as_matrix()\n",
    "Y = df_y.as_matrix()\n",
    "\n",
    "# --------------------\n",
    "# K-fold CV\n",
    "# --------------------\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, random_state=3)\n",
    "#ts = Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decision Tree (CART) Function\n",
    "\n",
    "\n",
    "def DT(maxdepth, ts):\n",
    "    # Train\n",
    "    dtc = DecisionTreeClassifier(max_depth = maxdepth)\n",
    "    dtc.fit(X_train, Y_train)\n",
    "\n",
    "    # Predicting\n",
    "    dtc_pred = dtc.predict(X_test)\n",
    "\n",
    "    # Finding mispredicted samples\n",
    "    dtc_verror = np.asarray([int(dtc_pred[i] != Y_test[i]) for i in range(0,ts)])\n",
    "    dtc_error = np.sum(dtc_verror)\n",
    "    dtc_ccidx = np.where(dtc_verror == 0)\n",
    "    dtc_mcidx = np.where(dtc_verror == 1)\n",
    "\n",
    "    perrordtc = float(dtc_error)/ts\n",
    "    return perrordtc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree (CART ) with Bagging/Boosting\n",
    "def bagb(maxdepth, nestimators, ts):\n",
    "    #Train\n",
    "    bagb = BaggingClassifier(DecisionTreeClassifier(max_depth=maxdepth), n_estimators=nestimators)\n",
    "    bagb.fit(X_train,Y_train)\n",
    "\n",
    "    # Predicting\n",
    "    bagb_pred = bagb.predict(X_test)\n",
    "\n",
    "    # Finding mispredicted samples\n",
    "    bagb_verror = np.asarray([int(bagb_pred[i] != Y_test[i]) for i in range(0,ts)])\n",
    "    bagb_error = np.sum(bagb_verror)\n",
    "    bagb_ccidx = np.where(bagb_verror == 0)\n",
    "    bagb_mcidx = np.where(bagb_verror == 1)\n",
    "\n",
    "    perrordtb = float(bagb_error)/ts\n",
    "    return perrordtb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Function (choose optimal gamma first)\n",
    "\n",
    "def gammachoice(X_train, Y_train, X_test, Y_test, ts, step):  \n",
    "    l=[]\n",
    "    for g in range(1,250, step):\n",
    "        svm_rbf = svm.SVC(kernel='rbf', gamma=float(g))\n",
    "        svm_rbf.fit(X_train,Y_train)\n",
    "        ypred_svm_rbf = svm_rbf.predict(X_test)\n",
    "        e_svm_rbf = float(np.sum((ypred_svm_rbf[i] != Y_test[i]) for i in range(0,ts)))\n",
    "        l.append((e_svm_rbf/ts))\n",
    "    plt.plot(range(0,250, step), l)\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('classification percent error')\n",
    "    minindex = l.index(min(l)) + 1\n",
    "    #tx = plt.xticks(range(0,250, step))\n",
    "    print \"optimal gamma is:\", step*minindex\n",
    "    return step*minindex\n",
    "\n",
    "def svmrbf(X_train, Y_train, X_test, Y_test, g, ts): \n",
    "    svm_rbf = svm.SVC(kernel='rbf', gamma=g)\n",
    "    svm_rbf.fit(X_train,Y_train)\n",
    "    \n",
    "    ypred_svm_rbf = svm_rbf.predict(X_test)\n",
    "    \n",
    "    e_svm_rbf = np.sum((ypred_svm_rbf[i] != Y_test[i]) for i in range(0,ts))\n",
    "    perrorsvmrbf = float(e_svm_rbf)/ts\n",
    "    return perrorsvmrbf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest (No Boosting Function)\n",
    "\n",
    "def RFreg(maxdepth, nestimators):\n",
    "    #Train\n",
    "    rdf = RandomForestClassifier(max_depth=maxdepth, n_estimators=nestimators, bootstrap = True)\n",
    "    rdf.fit(X_train,Y_train)\n",
    "\n",
    "    # Predicting\n",
    "    rdf_pred = rdf.predict(X_test)\n",
    "\n",
    "    # Finding mispredicted samples\n",
    "    rdf_verror = np.asarray([int(rdf_pred[i] != Y_test[i]) for i in range(0,ts)])\n",
    "    rdf_error = np.sum(rdf_verror)\n",
    "    rdf_ccidx = np.where(rdf_verror == 0)\n",
    "    rdf_mcidx = np.where(rdf_verror == 1)\n",
    "\n",
    "    perrorrf = float(rdf_error)/ts\n",
    "    return perrorrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest (Boosting Function)\n",
    "\n",
    "def RFboost(maxdepth, nestimators):\n",
    "    #Train\n",
    "    rdfb = AdaBoostClassifier(RandomForestClassifier(max_depth=maxdepth, n_estimators=nestimators, bootstrap = True))\n",
    "    rdfb.fit(X_train,Y_train)\n",
    "\n",
    "    # Predicting\n",
    "    rdf_pred = rdfb.predict(X_test)\n",
    "\n",
    "    # Finding mispredicted samples\n",
    "    rdf_verror = np.asarray([int(rdf_pred[i] != Y_test[i]) for i in range(0,ts)])\n",
    "    rdf_error = np.sum(rdf_verror)\n",
    "    rdf_ccidx = np.where(rdf_verror == 0)\n",
    "    rdf_mcidx = np.where(rdf_verror == 1)\n",
    "\n",
    "    perrorrfb = float(rdf_error)/ts\n",
    "    return perrorrfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svml(X_train, Y_train, X_test, Y_test, ts):  \n",
    "    svm_linear = svm.SVC(kernel='linear')\n",
    "    svm_linear.fit(X_train,Y_train)\n",
    "\n",
    "    ypred_svm_linear = svm_linear.predict(X_test)\n",
    "\n",
    "    e_svm_linear = np.sum((ypred_svm_linear[i] != Y_test[i]) for i in range(0,ts))\n",
    "    pererrorsvml = float(e_svm_linear)/ts\n",
    "    return perrorsvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of data in the training set is: 10.0\n",
      "best DT error is 0.330386213091\n",
      "best BAGDT error is 0.330386213091\n",
      "best RF error is 0.390423262602\n",
      "best Boosted RF error is 0.360278432693\n",
      "best DT error is 0.329431907488\n",
      "best BAGDT error is 0.328674076569\n",
      "best RF error is 0.326990007859\n",
      "best BAGDT error is 0.32272370046\n",
      "best BAGDT error is 0.322022005164\n",
      "best DT error is 0.327495228472\n",
      "best BAGDT error is 0.316548781857\n",
      "best BAGDT error is 0.316183900303\n",
      "best RF error is 0.319467834288\n",
      "best Boosted RF error is 0.338806556641\n",
      "best BAGDT error is 0.315734815314\n",
      "best RF error is 0.31031772763\n",
      "best Boosted RF error is 0.335466487033\n",
      "best BAGDT error is 0.314948916582\n",
      "best BAGDT error is 0.314415628158\n",
      "percent of data in the training set is: 20.0\n",
      "best DT error is 0.322681486627\n",
      "best BAGDT error is 0.313429536771\n",
      "best BAGDT error is 0.310840253876\n",
      "best BAGDT error is 0.307840474912\n",
      "best RF error is 0.310208721463\n",
      "best Boosted RF error is 0.325397076005\n",
      "percent of data in the training set is: 30.0\n",
      "best DT error is 0.321713399011\n",
      "best BAGDT error is 0.30403089026\n",
      "best RF error is 0.303056547941\n",
      "best Boosted RF error is 0.297174407275\n",
      "best BAGDT error is 0.301504817581\n",
      "best RF error is 0.296452672224\n",
      "best Boosted RF error is 0.290967485836\n",
      "best BAGDT error is 0.301107863303\n",
      "best DT error is 0.320594709682\n",
      "best BAGDT error is 0.300169607737\n",
      "best RF error is 0.29569485042\n",
      "best Boosted RF error is 0.274042798889\n",
      "best BAGDT error is 0.299700479954\n",
      "best RF error is 0.294576161091\n",
      "best BAGDT error is 0.296597019234\n",
      "percent of data in the training set is: 40.0\n",
      "best DT error is 0.31942573257\n",
      "best BAGDT error is 0.289365106096\n",
      "best RF error is 0.284691815426\n",
      "best BAGDT error is 0.285197036039\n",
      "best RF error is 0.280397440216\n",
      "best Boosted RF error is 0.258757157292\n",
      "best BAGDT error is 0.283933984507\n",
      "best RF error is 0.275387335803\n",
      "best Boosted RF error is 0.257873021219\n",
      "best BAGDT error is 0.282965644998\n",
      "percent of data in the training set is: 50.0\n",
      "best DT error is 0.313141009448\n",
      "percent of data in the training set is: 60.0\n",
      "best DT error is 0.312661825071\n",
      "best DT error is 0.310009472687\n",
      "best DT error is 0.305399431639\n",
      "best DT error is 0.302368171771\n",
      "best BAGDT error is 0.282917587622\n",
      "best BAGDT error is 0.279444269024\n",
      "best RF error is 0.272308178087\n",
      "best Boosted RF error is 0.232333438585\n",
      "percent of data in the training set is: 70.0\n",
      "percent of data in the training set is: 80.0\n",
      "percent of data in the training set is: 90.0\n",
      "best DT error is 0.299823187674\n",
      "best DT error is 0.298560242485\n",
      "best BAGDT error is 0.279110886588\n",
      "best DT error is 0.296539530184\n",
      "best BAGDT error is 0.275322051023\n",
      "best RF error is 0.267996968932\n",
      "best Boosted RF error is 0.229856024249\n",
      "best BAGDT error is 0.273806516797\n",
      "best RF error is 0.265723667593\n",
      "best Boosted RF error is 0.205860065673\n",
      "best BAGDT error is 0.267239201819\n"
     ]
    }
   ],
   "source": [
    "#For Loop to determine the best parameters - training size, maxdepth and nestimators\n",
    "\n",
    "\n",
    "splits = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "maxdepthoption = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "nestimatoroptions = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "bestDTerror = 1\n",
    "bestDTsplit = 0\n",
    "bestDTdepth = 0\n",
    "bestbagerror = 1\n",
    "bestbagsplit = 0\n",
    "bestbagdepth = 0\n",
    "bestbagestimator = 0\n",
    "bestRFerror = 1\n",
    "bestRFsplit = 0\n",
    "bestRFdepth = 0\n",
    "bestRFestimator = 0\n",
    "bestRFBerror = 1\n",
    "bestRFBsplit = 0\n",
    "bestRFBdepth = 0\n",
    "bestRFBestimator = 0\n",
    "bestSVMerror = 1\n",
    "bestSVMsplit = 0\n",
    "\n",
    "\n",
    "for i in splits:\n",
    "    print \"percent of data in the training set is:\", i*100\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=i, random_state=123)\n",
    "    ts = Y_test.shape[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for j in maxdepthoption:\n",
    "        DTE = DT(j, ts)\n",
    "        if DTE < bestDTerror:\n",
    "            bestDTerror = DTE\n",
    "            bestDTsplit = i\n",
    "            bestDTdepth = j\n",
    "        else:\n",
    "            continue\n",
    "        print \"best DT error is\", bestDTerror\n",
    "        \n",
    "        for k in nestimatoroptions:\n",
    "            BAGE = bagb(j, k, ts)\n",
    "            if BAGE < bestbagerror:\n",
    "                bestbagerror = BAGE\n",
    "                bestbagsplit = i\n",
    "                bestbagdepth = j\n",
    "                bestbagestimator = k\n",
    "            else:\n",
    "                continue\n",
    "            print \"best BAGDT error is\", bestbagerror\n",
    "            \n",
    "            RFE = RFreg(j, k)\n",
    "            if RFE < bestRFerror:\n",
    "                bestRFerror = RFE\n",
    "                bestRFsplit = i\n",
    "                bestRFdepth = j\n",
    "                bestRFestimator = k\n",
    "            else:\n",
    "                continue\n",
    "            print \"best RF error is\", bestRFerror\n",
    "            \n",
    "            RFBE = RFboost(j, k)\n",
    "            if RFBE < bestRFBerror:\n",
    "                bestRFBerror = RFBE\n",
    "                bestRFBsplit = i\n",
    "                bestRFBdepth = j\n",
    "                bestRFBestimator = k\n",
    "            else:\n",
    "                continue\n",
    "            print \"best Boosted RF error is\", bestRFBerror\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.296539530184\n"
     ]
    }
   ],
   "source": [
    "print bestDTerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.267239201819\n"
     ]
    }
   ],
   "source": [
    "print bestbagerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265723667593\n"
     ]
    }
   ],
   "source": [
    "print bestRFerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.205860065673\n"
     ]
    }
   ],
   "source": [
    "print bestRFBerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of data in the training set is: 10.0\n"
     ]
    }
   ],
   "source": [
    "svml = []\n",
    "for i in splits:\n",
    "    print \"percent of data in the training set is:\", i*100\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=i, random_state=123)\n",
    "    ts = Y_test.shape[0]\n",
    "    svm_linear = svm.SVC(kernel='linear')\n",
    "    svm_linear.fit(X_train,Y_train)\n",
    "\n",
    "    ypred_svm_linear = svm_linear.predict(X_test)\n",
    "\n",
    "    e_svm_linear = np.sum((ypred_svm_linear[i] != Y_test[i]) for i in range(0,ts))\n",
    "    pesl = float(e_svm_linear)/ts\n",
    "    svml.append(pesl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print svml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM - SVM will use its own loop because it has different parameters than the other models \n",
    "\n",
    "svmrbf = []\n",
    "\n",
    "for i in splits:\n",
    "    print \"percent of data in the training set is:\", i*100\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=i, random_state=123)\n",
    "    ts = Y_test.shape[0]\n",
    "    \n",
    "    \n",
    "    #choose gamma\n",
    "    step = 10\n",
    "    g = gammachoice(X_train, Y_train, X_test, Y_test, ts, step)\n",
    "    print g\n",
    "    plt.legend(splits, bbox_to_anchor=(1.25, 1.0))\n",
    "    \n",
    "    #run svm rbf with that split and optimal gamma and append percent error to the svmrbf array\n",
    "    svm_rbf = svm.SVC(kernel='rbf', gamma=g)\n",
    "    svm_rbf.fit(X_train,Y_train)\n",
    "    \n",
    "    ypred_svm_rbf = svm_rbf.predict(X_test)\n",
    "    \n",
    "    e_svm_rbf = np.sum((ypred_svm_rbf[i] != Y_test[i]) for i in range(0,ts))\n",
    "    perbf = float(e_svm_rbf)/ts\n",
    "    svmrbf.append(perbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
